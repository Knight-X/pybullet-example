{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from env.balancebot_env import BalancebotEnv\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines.common.policies import ActorCriticPolicy, FeedForwardPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.a2c.utils import linear\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.balancebot import BalanceBot\n",
    "from env.balancebot_env import BalancebotEnv\n",
    "import numpy as np\n",
    "import pybullet as p\n",
    "import os\n",
    "p.connect(p.DIRECT)\n",
    "random_init_angle = [np.random.uniform(-0.05, 0.05),\n",
    "                        np.random.uniform(-0.01, 0.01),\n",
    "                        np.random.uniform(-0.05, 0.05)]\n",
    "\n",
    "random_init_orient = p.getQuaternionFromEuler(random_init_angle)\n",
    "urdf_file = os.path.join(\"/Users/kazami/workspace/pybullet-example/env/\", \"urdfs/balancebot_simple.urdf\")\n",
    "balancebot_id = p.loadURDF(urdf_file,\n",
    "                            basePosition=[0, 0, 0.01],\n",
    "                            baseOrientation=random_init_orient)\n",
    "p.getDynamicsInfo(0, 1)\n",
    "env = BalancebotEnv(render=False)\n",
    "env.reset()\n",
    "\n",
    "b = env.balancebot\n",
    "print(b.getState(-1))\n",
    "b.changeBaseDynamics()\n",
    "print(b.getState(-1))\n",
    "print(b.getState(0))\n",
    "b.changeRightWheelDynamics()\n",
    "print(b.getState(0))\n",
    "print(b.getState(1))\n",
    "b.changeLeftWheelDynamics()\n",
    "print(b.getState(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.balancebot import BalanceBot\n",
    "import pybullet as p\n",
    "import os\n",
    "import numpy as np\n",
    "p.connect(p.GUI)\n",
    "random_init_angle = [np.random.uniform(-0.05, 0.05),\n",
    "                        np.random.uniform(-0.01, 0.01),\n",
    "                        np.random.uniform(-0.05, 0.05)]\n",
    "\n",
    "random_init_orient = p.getQuaternionFromEuler(random_init_angle)\n",
    "urdf_file = os.path.join(\"/Users/kazami/workspace/pybullet-example/env/\", \"balancebot_simple.urdf\")\n",
    "balancebot_id = p.loadURDF(urdf_file,\n",
    "                            basePosition=[0, 0, 0.01],\n",
    "                            baseOrientation=random_init_orient)\n",
    "\n",
    "while (1):\n",
    "    p.stepSimulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.balancebot import BalanceBot\n",
    "from env.balancebot_env import BalancebotEnv\n",
    "import numpy as np\n",
    "\n",
    "env = BalancebotEnv(render=False)\n",
    "env.reset()\n",
    "obs = np.array([[1.0, 2.0, 3.0, 4.0, 5.0, 2.0, 3.0, 1.0]], dtype=np.float32)\n",
    "cur_obs = np.tile(obs, [5, 1])\n",
    "cur_rwds = np.zeros(5)\n",
    "random_action_sequences = np.random.uniform(-1, 1, size=[5, 5, 1])\n",
    "a = env.mb_reward(cur_obs[0], random_action_sequences[:,0,:])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"model/gym/{}\".format(int(time.time()))\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "import numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "def make_env(rank):\n",
    "    def _init():\n",
    "        env = BalancebotEnv(render=False)\n",
    "        env = Monitor(env, os.path.join(log_dir, str(rank)))\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "num_cpu = 16\n",
    "env = SubprocVecEnv([make_env(rank=i) for i in range(num_cpu)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def const_linear(input_tensor, scope, np_w, np_b):\n",
    "    in_shape = input_tensor.get_shape().as_list()\n",
    "    n_input = in_shape[1]\n",
    "    assert n_input == np_w.shape[0], \\\n",
    "        f\"incompatible weight shape: {in_shape} and {np_w.shape}\"\n",
    "    n_output = np_w.shape[1]\n",
    "    assert n_output == np_b.shape[0], \\\n",
    "        f'incompatible bias shape: {np_w.shape} and {np_b.shape}'\n",
    "    with tf.variable_scope(scope):\n",
    "        tf_w = tf.constant(np_w, name='w', dtype=input_tensor.dtype)\n",
    "        tf_b = tf.constant(np_b, name='b', dtype=input_tensor.dtype)\n",
    "        return tf.matmul(input_tensor, tf_w) + tf_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RL Agwnt\n",
    "class CustomPolicy2(ActorCriticPolicy):\n",
    "    np_pi_w = None\n",
    "    np_pi_b = None\n",
    "    np_vf_w = None\n",
    "    np_vf_b = None\n",
    "    var_layers = [16]\n",
    "    \n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False):\n",
    "        super(CustomPolicy, self).__init__(sess, ob_space, \n",
    "                                           ac_space, n_env, n_steps, \n",
    "                                           n_batch, n_lstm=256,\n",
    "                                           reuse=reuse, scale=False)\n",
    "        if any([self.np_pi_w is None,\n",
    "                self.np_pi_b is None,\n",
    "                self.np_vf_w is None,\n",
    "                self.np_vf_b is None]):\n",
    "            raise ValueError('Run CustomPolicy.set_weights first before using this CustomPolicy')\n",
    "        with tf.variable_scope('model', reuse=reuse):\n",
    "            activ = tf.tanh\n",
    "            processed_x = tf.layers.flatten(self.processed_x)\n",
    "            pi_h = activ(const_linear(processed_x, 'pi_fc0', self.np_pi_w, self.np_pi_b))\n",
    "            vf_h = activ(const_linear(processed_x, 'vf_fc0', self.np_vf_w, self.np_vf_b))\n",
    "            for i, layer_size in enumerate(self.var_layers, 1):\n",
    "                pi_h = activ(linear(pi_h, 'pi_fc' + str(i), n_hidden=layer_size, init_scale=np.sqrt(2)))\n",
    "                vf_h = activ(linear(vf_h, 'vf_fc' + str(i), n_hidden=layer_size, init_scale=np.sqrt(2)))\n",
    "            value_fn = linear(vf_h, 'vf', 1)\n",
    "            pi_latent = pi_h\n",
    "            vf_latent = vf_h\n",
    "\n",
    "            self.proba_distribution, self.policy, self.q_value = \\\n",
    "                self.pdtype.proba_distribution_from_latent(pi_latent, vf_latent, init_scale=0.01)\n",
    "\n",
    "        self.value_fn = value_fn\n",
    "        self.initial_state = None\n",
    "        self._setup_init()\n",
    "    \n",
    "    def step(self, obs, state=None, mask=None, deterministic=False):\n",
    "        if deterministic:\n",
    "            action, value, neglogp = self.sess.run([self.deterministic_action, self._value, self.neglogp],\n",
    "                                                   {self.obs_ph: obs})\n",
    "        else:\n",
    "            action, value, neglogp = self.sess.run([self.action, self._value, self.neglogp],\n",
    "                                                   {self.obs_ph: obs})\n",
    "        return action, value, self.initial_state, neglogp\n",
    "\n",
    "    def proba_step(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.policy_proba, {self.obs_ph: obs})\n",
    "\n",
    "    def value(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self._value, {self.obs_ph: obs})\n",
    "    \n",
    "    @classmethod\n",
    "    def custom_set_weights(cls, np_pi_w, np_pi_b, np_vf_w, np_vf_b, var_layers=None):\n",
    "        cls.np_pi_w = np_pi_w\n",
    "        cls.np_pi_b = np_pi_b\n",
    "        cls.np_vf_w = np_vf_w\n",
    "        cls.np_vf_b = np_vf_b\n",
    "        if var_layers:\n",
    "            cls.var_layers = var_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your pytorch numpy weights here\n",
    "CustomPolicy2.custom_set_weights(np.random.rand(4, 32), np.random.rand(32),\n",
    "                                np.random.rand(4, 32), np.random.rand(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPolicy(FeedForwardPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomPolicy, self).__init__(*args, **kwargs,\n",
    "                                           layers=[32, 8],\n",
    "                                           feature_extraction=\"mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO2(CustomPolicy, env, verbose=1, tensorboard_log=log_dir+\"/tensorboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_dim = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_encoder = torch.rand(observation_dim, 32,requires_grad=False).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_encoder.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I put the weight of encoder into model ?\n",
    "## I want to put the weight into 'pi_fc0' and 'vf_fc0' and set them to untrainable\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Network](assets/network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Save the agent\n",
    "model.learn(total_timesteps=2e7, tb_log_name=\"PPO2\")\n",
    "model.save(\"ppo_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import graph_util\n",
    "checkpoint = tf.train.get_checkpoint_state(\"model/20200711_32_8_3/\")\n",
    "checkpoint.model_checkpoint_path\n",
    "meta_file_path = checkpoint.model_checkpoint_path+\".meta\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_graph = tf.Graph()\n",
    "with restore_graph.as_default():\n",
    "    saver_new = tf.train.import_meta_graph(meta_file_path, clear_devices=True)\n",
    "restore_graph_def = restore_graph.as_graph_def()\n",
    "\n",
    "with tf.Session(graph=restore_graph) as sess:\n",
    "    saver_new.restore(sess, checkpoint.model_checkpoint_path)\n",
    "    out_graph_def = graph_util.convert_variables_to_constants(sess,\n",
    "                                                              restore_graph_def,\n",
    "                                                              [\"output/add\"])\n",
    "    summaryWriter = tf.summary.FileWriter('log/', out_graph_def)\n",
    "\n",
    "for a in out_graph_def.node:\n",
    "        #if a.op == \"Const\":\n",
    "        #    print(a.name)\n",
    "        #    print(type(a.attr['value']))\n",
    "        #    print(tf.make_ndarray(a.attr['value'].tensor))\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.gfile.GFile(\"model/result.pb\", \"wb\") as fid:\n",
    "    fid.write(out_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_graph = tf.Graph()\n",
    "new_graph_def = new_graph.as_graph_def()\n",
    "\n",
    "with tf.gfile.GFile(\"model/result.pb\", \"rb\") as rf:\n",
    "    new_graph_def.ParseFromString(rf.read())\n",
    "with new_graph.as_default():\n",
    "    tf.import_graph_def(new_graph_def, name=\"\")\n",
    "new_graph_def\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pi_fc0_w_np = ''\n",
    "pi_fc0_b_np = ''\n",
    "pi_fc1_w_np = ''\n",
    "pi_fc1_b_np = ''\n",
    "pi_w_np = ''\n",
    "pi_b_np = ''\n",
    "logstd_np = ''\n",
    "for a in new_graph_def.node:\n",
    "    if a.name == \"model/pi_fc0/w\":\n",
    "        print(tf.make_ndarray(a.attr['value'].tensor).shape)\n",
    "        pi_fc0_w = tf.make_ndarray(a.attr['value'].tensor)\n",
    "        pi_fc0_w = pi_fc0_w.flatten()\n",
    "        for x in pi_fc0_w:\n",
    "            pi_fc0_w_np += str(x) + \", \"\n",
    "    if a.name == \"model/pi_fc0/b\":\n",
    "        print(tf.make_ndarray(a.attr['value'].tensor).shape)\n",
    "        pi_fc0_b = tf.make_ndarray(a.attr['value'].tensor).flatten()\n",
    "        for el in pi_fc0_b:\n",
    "            pi_fc0_b_np += str(el) + \", \"\n",
    "    if a.name == \"model/pi_fc1/w\":\n",
    "        pi_fc1_w = tf.make_ndarray(a.attr['value'].tensor).flatten()\n",
    "        for el in pi_fc1_w:\n",
    "            pi_fc1_w_np += str(el) + \", \"\n",
    "    if a.name == \"model/pi_fc1/b\":\n",
    "        pi_fc1_b = tf.make_ndarray(a.attr['value'].tensor).flatten()\n",
    "        for el in pi_fc1_b:\n",
    "            pi_fc1_b_np += str(el) + \", \"\n",
    "    if a.name == \"model/pi/w\":\n",
    "        pi_w = tf.make_ndarray(a.attr['value'].tensor).flatten()\n",
    "        for el in pi_w:\n",
    "            pi_w_np += str(el) + \", \"\n",
    "    if a.name == \"model/pi/b\":\n",
    "        pi_b = tf.make_ndarray(a.attr['value'].tensor).flatten()\n",
    "        for el in pi_b:\n",
    "            pi_b_np += str(el) + \", \"\n",
    "    if a.name == \"model/pi/logstd\":\n",
    "        logstd = tf.make_ndarray(a.attr['value'].tensor).flatten()\n",
    "        for el in logstd:\n",
    "            logstd_np += str(el) + \", \"\n",
    "print(\"first\")\n",
    "print(pi_fc0_w_np)\n",
    "print(\"second\")\n",
    "print(pi_fc0_b_np)\n",
    "print(\"5\")\n",
    "print(pi_fc1_w_np)\n",
    "print(\"6\")\n",
    "print(pi_fc1_b_np)\n",
    "print(\"7\")\n",
    "print(pi_w_np)\n",
    "print(\" \")\n",
    "print(pi_b_np)\n",
    "print(\" \")\n",
    "print(logstd_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = new_graph.get_tensor_by_name(\"input/Ob:0\")\n",
    "output_tensor = new_graph.get_tensor_by_name(\"model/Tanh_2:0\")\n",
    "itensor = np.array([[1.0, 2.0, 3.0, 4.0, 5.0, 2.0, 3.0, 1.0]], dtype=np.float32)\n",
    "with tf.Session(graph=new_graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    pred = sess.run(output_tensor, \n",
    "                    feed_dict={input_tensor: itensor})\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir+\"/tensorboard/\"\n",
    "from tensorflow.compat.v1 import AttrValue as _AttrValue\n",
    "new_graph = tf.Graph()\n",
    "new_graph_def = new_graph.as_graph_def()\n",
    "\n",
    "with tf.gfile.GFile(\"model/202006200542/result.pb\", \"rb\") as rf:\n",
    "    new_graph_def.ParseFromString(rf.read())\n",
    "with new_graph.as_default():\n",
    "    tf.import_graph_def(new_graph_def, name=\"\")\n",
    "s = 5\n",
    "for a in new_graph_def.node:\n",
    "    if a.name == \"model/pi/b\":\n",
    "        s = a.attr['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can open tensorboard at terminal\n",
    "## For example:\n",
    "### tensorboard --logdir log_dir+\"/tensorboard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete trained model to demonstrate loading\n",
    "del model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the evaluation env\n",
    "env = DummyVecEnv([lambda: BalancebotEnv(render=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the trained agent\n",
    "model = PPO2.load(\"ppo_saveg\", env=env, policy=CustomPolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Enjoy trained agent\n",
    "\n",
    "for ep in range(1):\n",
    "    obs = env.reset()\n",
    "    dones = False\n",
    "    while not dones:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, dones, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
